Start job showui_desktop
WARNING:model.qwen2_vl.modeling_qwen2_vl:`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:02<00:00,  1.45s/it]
Found 197 lora modules: ['model.layers.0.self_attn.q_proj', 'model.layers.0.self_attn.k_proj', 'model.layers.0.self_attn.v_proj', 'model.layers.0.self_attn.o_proj', 'model.layers.0.mlp.gate_proj', 'model.layers.0.mlp.up_proj', 'model.layers.0.mlp.down_proj', 'model.layers.1.self_attn.q_proj', 'model.layers.1.self_attn.k_proj', 'model.layers.1.self_attn.v_proj', 'model.layers.1.self_attn.o_proj', 'model.layers.1.mlp.gate_proj', 'model.layers.1.mlp.up_proj', 'model.layers.1.mlp.down_proj', 'model.layers.2.self_attn.q_proj', 'model.layers.2.self_attn.k_proj', 'model.layers.2.self_attn.v_proj', 'model.layers.2.self_attn.o_proj', 'model.layers.2.mlp.gate_proj', 'model.layers.2.mlp.up_proj', 'model.layers.2.mlp.down_proj', 'model.layers.3.self_attn.q_proj', 'model.layers.3.self_attn.k_proj', 'model.layers.3.self_attn.v_proj', 'model.layers.3.self_attn.o_proj', 'model.layers.3.mlp.gate_proj', 'model.layers.3.mlp.up_proj', 'model.layers.3.mlp.down_proj', 'model.layers.4.self_attn.q_proj', 'model.layers.4.self_attn.k_proj', 'model.layers.4.self_attn.v_proj', 'model.layers.4.self_attn.o_proj', 'model.layers.4.mlp.gate_proj', 'model.layers.4.mlp.up_proj', 'model.layers.4.mlp.down_proj', 'model.layers.5.self_attn.q_proj', 'model.layers.5.self_attn.k_proj', 'model.layers.5.self_attn.v_proj', 'model.layers.5.self_attn.o_proj', 'model.layers.5.mlp.gate_proj', 'model.layers.5.mlp.up_proj', 'model.layers.5.mlp.down_proj', 'model.layers.6.self_attn.q_proj', 'model.layers.6.self_attn.k_proj', 'model.layers.6.self_attn.v_proj', 'model.layers.6.self_attn.o_proj', 'model.layers.6.mlp.gate_proj', 'model.layers.6.mlp.up_proj', 'model.layers.6.mlp.down_proj', 'model.layers.7.self_attn.q_proj', 'model.layers.7.self_attn.k_proj', 'model.layers.7.self_attn.v_proj', 'model.layers.7.self_attn.o_proj', 'model.layers.7.mlp.gate_proj', 'model.layers.7.mlp.up_proj', 'model.layers.7.mlp.down_proj', 'model.layers.8.self_attn.q_proj', 'model.layers.8.self_attn.k_proj', 'model.layers.8.self_attn.v_proj', 'model.layers.8.self_attn.o_proj', 'model.layers.8.mlp.gate_proj', 'model.layers.8.mlp.up_proj', 'model.layers.8.mlp.down_proj', 'model.layers.9.self_attn.q_proj', 'model.layers.9.self_attn.k_proj', 'model.layers.9.self_attn.v_proj', 'model.layers.9.self_attn.o_proj', 'model.layers.9.mlp.gate_proj', 'model.layers.9.mlp.up_proj', 'model.layers.9.mlp.down_proj', 'model.layers.10.self_attn.q_proj', 'model.layers.10.self_attn.k_proj', 'model.layers.10.self_attn.v_proj', 'model.layers.10.self_attn.o_proj', 'model.layers.10.mlp.gate_proj', 'model.layers.10.mlp.up_proj', 'model.layers.10.mlp.down_proj', 'model.layers.11.self_attn.q_proj', 'model.layers.11.self_attn.k_proj', 'model.layers.11.self_attn.v_proj', 'model.layers.11.self_attn.o_proj', 'model.layers.11.mlp.gate_proj', 'model.layers.11.mlp.up_proj', 'model.layers.11.mlp.down_proj', 'model.layers.12.self_attn.q_proj', 'model.layers.12.self_attn.k_proj', 'model.layers.12.self_attn.v_proj', 'model.layers.12.self_attn.o_proj', 'model.layers.12.mlp.gate_proj', 'model.layers.12.mlp.up_proj', 'model.layers.12.mlp.down_proj', 'model.layers.13.self_attn.q_proj', 'model.layers.13.self_attn.k_proj', 'model.layers.13.self_attn.v_proj', 'model.layers.13.self_attn.o_proj', 'model.layers.13.mlp.gate_proj', 'model.layers.13.mlp.up_proj', 'model.layers.13.mlp.down_proj', 'model.layers.14.self_attn.q_proj', 'model.layers.14.self_attn.k_proj', 'model.layers.14.self_attn.v_proj', 'model.layers.14.self_attn.o_proj', 'model.layers.14.mlp.gate_proj', 'model.layers.14.mlp.up_proj', 'model.layers.14.mlp.down_proj', 'model.layers.15.self_attn.q_proj', 'model.layers.15.self_attn.k_proj', 'model.layers.15.self_attn.v_proj', 'model.layers.15.self_attn.o_proj', 'model.layers.15.mlp.gate_proj', 'model.layers.15.mlp.up_proj', 'model.layers.15.mlp.down_proj', 'model.layers.16.self_attn.q_proj', 'model.layers.16.self_attn.k_proj', 'model.layers.16.self_attn.v_proj', 'model.layers.16.self_attn.o_proj', 'model.layers.16.mlp.gate_proj', 'model.layers.16.mlp.up_proj', 'model.layers.16.mlp.down_proj', 'model.layers.17.self_attn.q_proj', 'model.l
/home/qinghong/.local/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:543: UserWarning: Model with `tie_word_embeddings=True` and the tied_target_modules=['lm_head'] are part of the adapter. This can lead to complications, for example when merging the adapter or converting your model to formats other than safetensors. See for example https://github.com/huggingface/peft/issues/2018.
  warnings.warn(
trainable params: 41,840,640 || all params: 2,250,826,240 || trainable%: 1.8589
[Name] base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight  [Shape] torch.Size([1536, 32])
[Name] base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight  [Shape] torch.Size([256, 32])
[Name] base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight  [Shape] torch.Size([256, 32])
[Name] base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight  [Shape] torch.Size([1536, 32])
[Name] base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight  [Shape] torch.Size([8960, 32])
[Name] base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight  [Shape] torch.Size([8960, 32])
[Name] base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight  [Shape] torch.Size([32, 8960])
[Name] base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight  [Shape] torch.Size([1536, 32])
[Name] base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight  [Shape] torch.Size([1536, 32])
[Name] base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight  [Shape] torch.Size([256, 32])
[Name] base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight  [Shape] torch.Size([256, 32])
[Name] base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight  [Shape] torch.Size([1536, 32])
[Name] base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight  [Shape] torch.Size([8960, 32])
[Name] base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight  [Shape] torch.Size([8960, 32])
[Name] base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight  [Shape] torch.Size([32, 8960])
[Name] base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight  [Shape] torch.Size([1536, 32])
[Name] base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight  [Shape] torch.Size([1536, 32])
[Name] base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight  [Shape] torch.Size([256, 32])
[Name] base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight  [Shape] torch.Size([256, 32])
[Name] base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight  [Shape] torch.Size([1536, 32])
[Name] base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight  [Shape] torch.Size([8960, 32])
[Name] base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight  [Shape] torch.Size([8960, 32])
[Name] base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight  [Shape] torch.Size([32, 8960])
[Name] base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight  [Shape] torch.Size([1536, 32])
[Name] base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight  [Shape] torch.Size([1536, 32])
[Name] base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight  [Shape] torch.Size([256, 32])
[Name] base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight  [Shape] torch.Size([256, 32])
[Name] base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight  [Shape] torch.Size([1536, 32])
[Name] base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight  [Shape] torch.Size([8960, 32])
[Name] base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight  [Shape] torch.Size([8960, 32])
[Name] base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight  [Shape] torch.Size([32, 8960])
[Name] base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight  [Shape] torch.Size([1536, 32])
[Name] base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight  [Shape] torch.Size([1536, 32])
[Name] base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight  [Shape] torch.Size([256, 32])
[Name] base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight  [Shape] torch.Size([256, 32])
[Name] base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight  [Shape] torch.Size([1536, 32])
[Name] base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight  [Shape] torch.Size([8960, 32])
[Name] base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight  [Shape] torch.Size([8960, 32])
[Name] base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight  [Shape] torch.Size([32, 8960])
[Name] base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight  [Shape] torch.Size([1536, 32])
[Name] base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight  [Shape] torch.Size([1536, 32])
[Name] base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight  [Shape] torch.Size([256, 32])
[Name] base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight  [Shape] torch.Size([256, 32])
[Name] base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight  [Shape] torch.Size([1536, 32])
[Name] base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight  [Shape] torch.Size([8960, 32])
[Name] base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight  [Shape] torch.Size([8960, 32])
[Name] base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight  [Shape] torch.Size([32, 8960])
[Name] base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight  [Shape] torch.Size([1536, 32])
[Name] base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight  [Shape] torch.Size([1536, 32])
[Name] base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight  [Shape] torch.Size([256, 32])
[Name] base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight  [Shape] torch.Size([256, 32])
[Name] base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight  [Shape] torch.Size([1536, 32])
[Name] base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight  [Shape] torch.Size([8960, 32])
[Name] base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight  [Shape] torch.Size([8960, 32])
[Name] base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight  [Shape] torch.Size([32, 8960])
[Name] base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight  [Shape] torch.Size([1536, 32])
[Name] base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight  [Shape] torch.Size([1536, 32])
[Name] base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight  [Shape] torch.Size([256, 32])
[Name] base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight  [Shape] torch.Size([256, 32])
[Name] base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight  [Shape] torch.Size([1536, 32])
[Name] base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight  [Shape] torch.Size([8960, 32])
[Name] base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight  [Shape] torch.Size([8960, 32])
[Name] base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight  [Shape] torch.Size([32, 8960])
[Name] base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight  [Shape] torch.Size([1536, 32])
[Name] base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight  [Shape] torch.Size([1536, 32])
[Name] base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight  [Shape] torch.Size([256, 32])
[Name] base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight  [Shape] torch.Size([256, 32])
[Name] base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight  [Shape] torch.Size([1536, 32])
[Name] base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight  [Shape] torch.Size([8960, 32])
[Name] base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight  [Shape] torch.Size([8960, 32])
[Name] base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight  [Shape] torch.Size([32, 8960])
[Name] base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight  [Shape] torch.Size([1536, 32])
[Name] base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight  [Shape] torch.Size([1536, 32])
[Name] base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight  [Shape] torch.Size([256, 32])
[Name] base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight  [Shape] torch.Size([256, 32])
[Name] base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight  [Shape] torch.Size([1536, 32])
[Name] base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight  [Shape] torch.Size([8960, 32])
[Name] base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight  [Shape] torch.Size([8960, 32])
[Name] base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight  [Shape] torch.Size([32, 8960])
[Name] base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight  [Shape] torch.Size([1536, 32])
[Name] base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight  [Shape] torch.Size([1536, 32])
[Name] base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight  [Shape] torch.Size([256, 32])
[Name] base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight  [Shape] torch.Size([256, 32])
[Name] base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight  [Shape] torch.Size([1536, 32])
[Name] base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight  [Shape] torch.Size([8960, 32])
[Name] base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight  [Shape] torch.Size([8960, 32])
[Name] base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight  [Shape] torch.Size([32, 8960])
[Name] base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight  [Shape] torch.Size([1536, 32])
[Name] base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight  [Shape] torch.Size([1536, 32])
[Name] base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight  [Shape] torch.Size([256, 32])
[Name] base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight  [Shape] torch.Size([256, 32])
[Name] base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight  [Shape] torch.Size([1536, 32])
[Name] base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight  [Shape] torch.Size([8960, 32])
[Name] base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight  [Shape] torch.Size([8960, 32])
[Name] base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight  [Shape] torch.Size([32, 8960])
[Name] base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight  [Shape] torch.Size([1536, 32])
[Name] base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight  [Shape] torch.Size([1536, 32])
[Name] base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight  [Shape] torch.Size([256, 32])
[Name] base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight  [Shape] torch.Size([256, 32])
[Name] base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight  [Shape] torch.Size([1536, 32])
[Name] base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight  [Shape] torch.Size([8960, 32])
[Name] base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight  [Shape] torch.Size([8960, 32])
[Name] base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight  [Shape] torch.Size([32, 8960])
[Name] base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight  [Shape] torch.Size([1536, 32])
[Name] base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight  [Shape] torch.Size([1536, 32])
[Name] base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight  [Shape] torch.Size([256, 32])
[Name] base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight  [Shape] torch.Size([256, 32])
[Name] base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight  [Shape] torch.Size([1536, 32])
[Name] base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight  [Shape] torch.Size([8960, 32])
[Name] base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight  [Shape] torch.Size([8960, 32])
[Name] base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight  [Shape] torch.Size([32, 8960])
[Name] base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight  [Shape] torch.Size([1536, 32])
[Name] base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight  [Shape] torch.Size([1536, 32])
[Name] base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight  [Shape] torch.Size([256, 32])
[Name] base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight  [Shape] torch.Size([256, 32])
[Name] base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight  [Shape] torch.Size([1536, 32])
[Name] base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight  [Shape] torch.Size([8960, 32])
[Name] base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight  [Shape] torch.Size([8960, 32])
[Name] base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight  [Shape] torch.Size([32, 8960])
[Name] base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight  [Shape] torch.Size([1536, 32])
[Name] base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight  [Shape] torch.Size([1536, 32])
[Name] base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight  [Shape] torch.Size([256, 32])
[Name] base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight  [Shape] torch.Size([256, 32])
[Name] base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight  [Shape] torch.Size([1536, 32])
[Name] base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight  [Shape] torch.Size([8960, 32])
[Name] base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight  [Shape] torch.Size([8960, 32])
[Name] base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight  [Shape] torch.Size([32, 8960])
[Name] base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight  [Shape] torch.Size([1536, 32])
[Name] base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight  [Shape] torch.Size([1536, 32])
[Name] base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight  [Shape] torch.Size([256, 32])
[Name] base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight  [Shape] torch.Size([256, 32])
[Name] base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight  [Shape] torch.Size([1536, 32])
[Name] base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight  [Shape] torch.Size([8960, 32])
[Name] base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight  [Shape] torch.Size([8960, 32])
[Name] base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight  [Shape] torch.Size([32, 8960])
[Name] base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight  [Shape] torch.Size([1536, 32])
[Name] base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight  [Shape] torch.Size([1536, 32])
[Name] base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight  [Shape] torch.Size([256, 32])
[Name] base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight  [Shape] torch.Size([256, 32])
[Name] base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight  [Shape] torch.Size([1536, 32])
[Name] base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight  [Shape] torch.Size([8960, 32])
[Name] base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight  [Shape] torch.Size([8960, 32])
[Name] base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight  [Shape] torch.Size([32, 8960])
[Name] base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight  [Shape] torch.Size([1536, 32])
[Name] base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight  [Shape] torch.Size([1536, 32])
[Name] base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight  [Shape] torch.Size([256, 32])
[Name] base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight  [Shape] torch.Size([256, 32])
[Name] base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight  [Shape] torch.Size([1536, 32])
[Name] base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight  [Shape] torch.Size([8960, 32])
[Name] base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight  [Shape] torch.Size([8960, 32])
[Name] base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight  [Shape] torch.Size([32, 8960])
[Name] base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight  [Shape] torch.Size([1536, 32])
[Name] base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight  [Shape] torch.Size([1536, 32])
[Name] base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight  [Shape] torch.Size([256, 32])
[Name] base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight  [Shape] torch.Size([256, 32])
[Name] base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight  [Shape] torch.Size([1536, 32])
[Name] base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight  [Shape] torch.Size([8960, 32])
[Name] base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight  [Shape] torch.Size([8960, 32])
[Name] base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight  [Shape] torch.Size([32, 8960])
[Name] base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight  [Shape] torch.Size([1536, 32])
[Name] base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight  [Shape] torch.Size([1536, 32])
[Name] base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight  [Shape] torch.Size([256, 32])
[Name] base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight  [Shape] torch.Size([256, 32])
[Name] base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight  [Shape] torch.Size([1536, 32])
[Name] base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight  [Shape] torch.Size([8960, 32])
[Name] base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight  [Shape] torch.Size([8960, 32])
[Name] base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight  [Shape] torch.Size([32, 8960])
[Name] base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight  [Shape] torch.Size([1536, 32])
[Name] base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight  [Shape] torch.Size([1536, 32])
[Name] base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight  [Shape] torch.Size([256, 32])
[Name] base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight  [Shape] torch.Size([256, 32])
[Name] base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight  [Shape] torch.Size([1536, 32])
[Name] base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight  [Shape] torch.Size([8960, 32])
[Name] base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight  [Shape] torch.Size([8960, 32])
[Name] base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight  [Shape] torch.Size([32, 8960])
[Name] base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight  [Shape] torch.Size([1536, 32])
[Name] base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight  [Shape] torch.Size([1536, 32])
[Name] base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight  [Shape] torch.Size([256, 32])
[Name] base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight  [Shape] torch.Size([256, 32])
[Name] base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight  [Shape] torch.Size([1536, 32])
[Name] base_model.model.model.layers.22.mlp.gate_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.22.mlp.gate_proj.lora_B.default.weight  [Shape] torch.Size([8960, 32])
[Name] base_model.model.model.layers.22.mlp.up_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.22.mlp.up_proj.lora_B.default.weight  [Shape] torch.Size([8960, 32])
[Name] base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight  [Shape] torch.Size([32, 8960])
[Name] base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight  [Shape] torch.Size([1536, 32])
[Name] base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight  [Shape] torch.Size([1536, 32])
[Name] base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight  [Shape] torch.Size([256, 32])
[Name] base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight  [Shape] torch.Size([256, 32])
[Name] base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight  [Shape] torch.Size([1536, 32])
[Name] base_model.model.model.layers.23.mlp.gate_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.23.mlp.gate_proj.lora_B.default.weight  [Shape] torch.Size([8960, 32])
[Name] base_model.model.model.layers.23.mlp.up_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.23.mlp.up_proj.lora_B.default.weight  [Shape] torch.Size([8960, 32])
[Name] base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight  [Shape] torch.Size([32, 8960])
[Name] base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight  [Shape] torch.Size([1536, 32])
[Name] base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight  [Shape] torch.Size([1536, 32])
[Name] base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight  [Shape] torch.Size([256, 32])
[Name] base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight  [Shape] torch.Size([256, 32])
[Name] base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight  [Shape] torch.Size([1536, 32])
[Name] base_model.model.model.layers.24.mlp.gate_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.24.mlp.gate_proj.lora_B.default.weight  [Shape] torch.Size([8960, 32])
[Name] base_model.model.model.layers.24.mlp.up_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.24.mlp.up_proj.lora_B.default.weight  [Shape] torch.Size([8960, 32])
[Name] base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight  [Shape] torch.Size([32, 8960])
[Name] base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight  [Shape] torch.Size([1536, 32])
[Name] base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight  [Shape] torch.Size([1536, 32])
[Name] base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight  [Shape] torch.Size([256, 32])
[Name] base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight  [Shape] torch.Size([256, 32])
[Name] base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight  [Shape] torch.Size([1536, 32])
[Name] base_model.model.model.layers.25.mlp.gate_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.25.mlp.gate_proj.lora_B.default.weight  [Shape] torch.Size([8960, 32])
[Name] base_model.model.model.layers.25.mlp.up_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.25.mlp.up_proj.lora_B.default.weight  [Shape] torch.Size([8960, 32])
[Name] base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight  [Shape] torch.Size([32, 8960])
[Name] base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight  [Shape] torch.Size([1536, 32])
[Name] base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight  [Shape] torch.Size([1536, 32])
[Name] base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight  [Shape] torch.Size([256, 32])
[Name] base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight  [Shape] torch.Size([256, 32])
[Name] base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight  [Shape] torch.Size([1536, 32])
[Name] base_model.model.model.layers.26.mlp.gate_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.26.mlp.gate_proj.lora_B.default.weight  [Shape] torch.Size([8960, 32])
[Name] base_model.model.model.layers.26.mlp.up_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.26.mlp.up_proj.lora_B.default.weight  [Shape] torch.Size([8960, 32])
[Name] base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight  [Shape] torch.Size([32, 8960])
[Name] base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight  [Shape] torch.Size([1536, 32])
[Name] base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight  [Shape] torch.Size([1536, 32])
[Name] base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight  [Shape] torch.Size([256, 32])
[Name] base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight  [Shape] torch.Size([256, 32])
[Name] base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight  [Shape] torch.Size([1536, 32])
[Name] base_model.model.model.layers.27.mlp.gate_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.27.mlp.gate_proj.lora_B.default.weight  [Shape] torch.Size([8960, 32])
[Name] base_model.model.model.layers.27.mlp.up_proj.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.model.layers.27.mlp.up_proj.lora_B.default.weight  [Shape] torch.Size([8960, 32])
[Name] base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight  [Shape] torch.Size([32, 8960])
[Name] base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight  [Shape] torch.Size([1536, 32])
[Name] base_model.model.lm_head.lora_A.default.weight  [Shape] torch.Size([32, 1536])
[Name] base_model.model.lm_head.lora_B.default.weight  [Shape] torch.Size([151936, 32])
Loading 1 Training Datasets
Traceback (most recent call last):
  File "/home/qinghong/showui_public/ShowUI/train.py", line 749, in <module>
    main(sys.argv[1:])
  File "/home/qinghong/showui_public/ShowUI/train.py", line 460, in main
    train_dataset = HybridDataset(
  File "/home/qinghong/showui_public/ShowUI/data/dataset.py", line 255, in __init__
    GroundingDataset(
  File "/home/qinghong/showui_public/ShowUI/data/dset_shared_grounding.py", line 166, in __init__
    with open(os.path.join(META_DIR, "{}.json".format(json_data))) as f:
FileNotFoundError: [Errno 2] No such file or directory: '$_DATA_DIR/ShowUI-desktop/metadata/hf_train.json'
Traceback (most recent call last):
  File "/home/qinghong/showui_public/ShowUI/train.py", line 749, in <module>
    main(sys.argv[1:])
  File "/home/qinghong/showui_public/ShowUI/train.py", line 460, in main
    train_dataset = HybridDataset(
  File "/home/qinghong/showui_public/ShowUI/data/dataset.py", line 255, in __init__
    GroundingDataset(
  File "/home/qinghong/showui_public/ShowUI/data/dset_shared_grounding.py", line 166, in __init__
    with open(os.path.join(META_DIR, "{}.json".format(json_data))) as f:
FileNotFoundError: [Errno 2] No such file or directory: '$_DATA_DIR/ShowUI-desktop/metadata/hf_train.json'
